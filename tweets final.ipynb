{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter US airline sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we examine tweets about US airlines.\n",
    "It is a popular dataset, description of which can be found at https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "\n",
    "Our aim is twofold: \n",
    "* to write a **classifier which uses tweet's body only to classify a given tweet as positive, negative or neutral,**\n",
    "* to **use twitter API** to fetch some recent tweets about airlines and **test our classifier in the real world.**\n",
    "\n",
    "We write our **own classes for feature extraction**, in particular we **extract emojis and exclamation marks** (both single and multiple) from tweets, as well as perform standard tokenization and stemming using nltk library.\n",
    "\n",
    "**Our classes import from sklearn mixin classes, so that we can use them together with sklearn's pipelines and grid search.**\n",
    "\n",
    "Withour further ado, let's begin necessary imports. Note a dependecy on a non standard emoji library (pip install emoji --upgrade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# regex\n",
    "import re\n",
    "\n",
    "# library for emojis regex\n",
    "import emoji\n",
    "\n",
    "# nltk import\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#nltk.download()\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# clasifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# twitter\n",
    "from twitter import Twitter, OAuth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's read in and peek at our raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use solely the tweet's body (text) to predict it's sentiment: positive, negative or neutral.\n",
    "Because of that, we don't inspect any of the other columns of the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of the sentiments for each airline and in total. \n",
    "\n",
    "First let's look at raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>American</th>\n",
       "      <td>1960.0</td>\n",
       "      <td>463.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>2759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta</th>\n",
       "      <td>955.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>2222.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southwest</th>\n",
       "      <td>1186.0</td>\n",
       "      <td>664.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>2420.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US Airways</th>\n",
       "      <td>2263.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>2913.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United</th>\n",
       "      <td>2633.0</td>\n",
       "      <td>697.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>3822.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virgin America</th>\n",
       "      <td>181.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>504.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>9178.0</td>\n",
       "      <td>3099.0</td>\n",
       "      <td>2363.0</td>\n",
       "      <td>14640.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "airline_sentiment  negative  neutral  positive    Total\n",
       "airline                                                \n",
       "American             1960.0    463.0     336.0   2759.0\n",
       "Delta                 955.0    723.0     544.0   2222.0\n",
       "Southwest            1186.0    664.0     570.0   2420.0\n",
       "US Airways           2263.0    381.0     269.0   2913.0\n",
       "United               2633.0    697.0     492.0   3822.0\n",
       "Virgin America        181.0    171.0     152.0    504.0\n",
       "Total                9178.0   3099.0    2363.0  14640.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot  = raw_data.pivot_table(values='tweet_id', index='airline', columns = 'airline_sentiment' ,\n",
    "                      aggfunc = 'count', margins=True, margins_name = 'Total')\n",
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, let's look at percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>American</th>\n",
       "      <td>0.710402</td>\n",
       "      <td>0.167814</td>\n",
       "      <td>0.121783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta</th>\n",
       "      <td>0.429793</td>\n",
       "      <td>0.325383</td>\n",
       "      <td>0.244824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southwest</th>\n",
       "      <td>0.490083</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>0.235537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US Airways</th>\n",
       "      <td>0.776862</td>\n",
       "      <td>0.130793</td>\n",
       "      <td>0.092345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United</th>\n",
       "      <td>0.688906</td>\n",
       "      <td>0.182365</td>\n",
       "      <td>0.128728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virgin America</th>\n",
       "      <td>0.359127</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.301587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.626913</td>\n",
       "      <td>0.211680</td>\n",
       "      <td>0.161407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "airline_sentiment  negative   neutral  positive\n",
       "airline                                        \n",
       "American           0.710402  0.167814  0.121783\n",
       "Delta              0.429793  0.325383  0.244824\n",
       "Southwest          0.490083  0.274380  0.235537\n",
       "US Airways         0.776862  0.130793  0.092345\n",
       "United             0.688906  0.182365  0.128728\n",
       "Virgin America     0.359127  0.339286  0.301587\n",
       "Total              0.626913  0.211680  0.161407"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot['negative'] = pivot['negative']/pivot['Total']\n",
    "pivot['neutral'] = pivot['neutral']/pivot['Total']\n",
    "pivot['positive'] = pivot['positive']/pivot['Total']\n",
    "pivot.drop('Total', axis = 1, inplace = True)\n",
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that tweets aimed at the airlines are used mainly for complaints. US Airways had the highest percentage of negative tweets (although, in absolute terms, the glorious first place goes to United). At the other end of the spectrum is Virgin America, which has both the smallest absolute number of tweets (by a factor of 4!) and the smallest proportion of negative tweets. In fact, for Virgin, the split between the three classes is almost equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful observation is that our dataset is heavily skewed for negative tweets. This will play a role in training of our classifiers - care will be taken to make sure classifiers can recognize less frequent classes (positive and neutral) as well as the most prominent type of tweets (negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract the series containing tweets' bodies and associated sentiments, which will serve as our X (data) and y (labels) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_tweets = raw_data['text']\n",
    "raw_sentiment = raw_data['airline_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map the sentiments to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = raw_sentiment.map({'negative': 0, 'positive': 1, 'neutral' : 2}).values\n",
    "X = raw_tweets.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's split our data into training (80%) and testing (20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                            random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction: custom classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write four custom classes for extracting features from text:\n",
    "* ExclamationMarksExtractor to extract single or multiple exclamation marks (we differentiate between '!' and '!!!!!')\n",
    "* KeyboardEmojisExtractor to extract emojis like :), :( or :D\n",
    "* GraphicsEmojisExtractor to extract emojis like ðŸ˜‚ or ðŸ˜€\n",
    "* TextCleaner, which removes tweeter mentions (@Airline), replaces any hyperlinks with the word URL, removes punctuation and stop words, lowercases everything and finally performs tokenization and stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A note on hashtags:* We decide against separately extracting hashtags; instead, # sign is stripped by the TextCleaner class and remaining words become part of the vocabulary for Count Vectorizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A note on regeular expressions:* In the classes below, given a repeated application of the same regular expressions, it would have been advantageous to compile regexes once at the beginning, for improved performance. However, this leads to problems when using sklearn's GridSearchCV, as regexes are not deep copyable. Because of that, we decided to store our regexes as strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a string representation of a regex for graphical emojis, we modified slightly a function from the emoji library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this piece of code is a slight modification of a function of the same name from \n",
    "# https://github.com/carpedm20/emoji/blob/master/emoji/core.py\n",
    "# essentially, we want the funciton to return a string, not a compiled regex\n",
    "\n",
    "def get_emoji_regexp():\n",
    "    '''Returns a string representing a regular expression caputing all grapical emojis.'''\n",
    "    \n",
    "    emojis = sorted(emoji.EMOJI_UNICODE.values(), key=len,\n",
    "                        reverse=True)\n",
    "    emoji_regex = u'(' + u'|'.join(re.escape(u) for u in emojis) + u')'\n",
    "    \n",
    "    return emoji_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExclamationMarksExtractor(BaseEstimator, TransformerMixin):\n",
    "    '''Class for extracting single or multiple exclamation marks from strings.'''\n",
    "    \n",
    "    def __init__(self, r1 =  r'(?<!\\!)!(?!\\!)', r2 = r'!{2,}'):\n",
    "        '''Args: \n",
    "            r1 (str): regex for a single exclamation mark\n",
    "            r2 (str): regex for two or more exclamation mars\n",
    "        '''\n",
    "        \n",
    "        self.r1 = r1 \n",
    "        self.r2 = r2\n",
    "    \n",
    "    def fit(self,x, y = None):\n",
    "        ''' Fit method required by sklearn'''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        ''' \n",
    "        Args:\n",
    "            text: list of strings\n",
    "        \n",
    "        Returns:\n",
    "            A list of transformed strings, ie a list of strings consisting of '!' for \n",
    "            each occurence of a single exclamation mark and '!!' for two or more \n",
    "            exclamation marks, joined by spaces. \n",
    "            \n",
    "            eg: 'I loved it! Can't wait to fly United again!!!!!!' is transformed to '! !!'.\n",
    "        '''\n",
    "        \n",
    "        # find all single or multiple exclamation marks\n",
    "        exclamation = [re.findall(self.r1,tweet) + re.findall(self.r2,tweet) for tweet in text]\n",
    "        \n",
    "        #joins lists back into single strings\n",
    "        exclamation = [' '.join(tweet) for tweet in exclamation]\n",
    "        \n",
    "        #represent 2 or more '!'s with '!!'\n",
    "        exclamation = [re.sub(self.r2,'!!',tweet) for tweet in exclamation]\n",
    "        \n",
    "        return exclamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KeyboardEmojisExtractor(BaseEstimator, TransformerMixin):\n",
    "    '''Class for extracting keyboard emojis, like :) or :P'''\n",
    "    \n",
    "    def __init__(self, emojis_regexp_list = [r':\\)', r':\\(', r':-D', r':P', r';\\)', r';P', r';D',\n",
    "                              r';\\)', r'xD', r'XD', r':-\\)',r':-\\(', r':-P', r':\\\\',\n",
    "                              r':\\/', r':D', r':-\\(']):\n",
    "        '''\n",
    "        Args:\n",
    "            emojis_regex_list (list of str): list of regexes for extraction of keyboard emojis \n",
    "        '''\n",
    "        \n",
    "        self.emojis_regexp_list = emojis_regexp_list \n",
    "    \n",
    "    \n",
    "    def fit(self, x, y = None):\n",
    "        ''' Fit method required by sklearn'''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        ''' \n",
    "        Args:\n",
    "            text: list of strings\n",
    "        \n",
    "        Returns:\n",
    "            A list of transformed strings, ie a list of strings consisting of keyboard emojis \n",
    "            found in tweets, joined by spaces. \n",
    "            \n",
    "            eg: 'I loved it! :):) is transformed to ':) :)'.\n",
    "        '''\n",
    "        \n",
    "        # create list of empty lists to keep emojis found in each tweet\n",
    "        keybord_emojis = []\n",
    "        for i in range(len(text)):\n",
    "            keybord_emojis.append([])\n",
    "        \n",
    "        # iterate over all regexp and all tweets \n",
    "        # to find all emojis from the emojis_regex_list for each tweet\n",
    "        for rgxp in self.emojis_regexp_list:\n",
    "            for i in range(len(text)):\n",
    "                keybord_emojis[i] += re.findall(rgxp, text[i])\n",
    "        \n",
    "        # join into a format required by Count/Tfidf Vectorizer\n",
    "        keybord_emojis = [' '.join(tweet) for tweet in keybord_emojis]\n",
    "        \n",
    "        return keybord_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GraphicsEmojisExtractor(BaseEstimator, TransformerMixin):\n",
    "    '''Class for extracting graphics emojis'''  \n",
    "    \n",
    "    def __init__(self, r = get_emoji_regexp()):\n",
    "        ''' \n",
    "        Args:\n",
    "            r (str): regex for emoji extraction\n",
    "        '''\n",
    "        \n",
    "        self.r = r \n",
    "    \n",
    "    \n",
    "    def fit(self, x, y = None):\n",
    "        ''' Fit method required by sklearn'''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        '''\n",
    "        Args:\n",
    "            text: list of strings\n",
    "        \n",
    "        Returns:\n",
    "            A list of transformed strings, ie a list of strings consisting of \n",
    "            graphics emojis found in tweets, joined by spaces. \n",
    "        '''\n",
    "        \n",
    "        #extract graphics emojis\n",
    "        graphics_emojis = [re.findall(self.r, tweet) for tweet in text]\n",
    "        \n",
    "        #join into a format required by CountVectorizer\n",
    "        graphics_emojis = [' '.join(tweet) for tweet in graphics_emojis]\n",
    "        \n",
    "        return graphics_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    '''Class for cleaning, tokenizing and stemming text.'''\n",
    "    \n",
    "    def __init__(self, r1 = r'@\\w+', \n",
    "            r2 = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
    "            tokenizer = RegexpTokenizer(r'\\w+'), stemmer = PorterStemmer(),\n",
    "            stop_words = nltk.corpus.stopwords.words(\"english\")):\n",
    "        '''\n",
    "        Args:\n",
    "            r1 (str): regex for twitter mentions\n",
    "            r2 (str): regex for URL's\n",
    "            tokenizer: RegexpTokenizer\n",
    "            stemmer: nltk porter stemmer\n",
    "            stop_words: list of stop words\n",
    "        '''\n",
    "        \n",
    "        self.r1 = r1 \n",
    "        self.r2 = r2 \n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.stop_words = stop_words\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        ''' Fit method required by sklearn'''\n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        '''\n",
    "        Args:\n",
    "            text: list of strings\n",
    "        \n",
    "        Returns:\n",
    "            A list of transformed strings, ie a list of strings \n",
    "            consisting of stemmed lowercase tokens. \n",
    "        '''\n",
    "        \n",
    "        #replace all @airline_name mentions with empty string\n",
    "        tweets = [re.sub(self.r1,'',tweet) for tweet in text]\n",
    "        \n",
    "        #substitute any urf for a string 'url\n",
    "        tweets = [re.sub(self.r2,'url',tweet) for tweet in tweets]\n",
    "        \n",
    "        #make everything lowercase\n",
    "        tweets = [tweet.lower() for tweet in tweets]\n",
    "        \n",
    "        #tokenize\n",
    "        tweets = [self.tokenizer.tokenize(tweet) for tweet in tweets]\n",
    "        \n",
    "        #remove stop words\n",
    "        tweets = [[word for word in tweet if word not in self.stop_words] for tweet in tweets]\n",
    "        \n",
    "        #perform stemming\n",
    "        tweets = [[self.stemmer.stem(word) for word in tweet] for tweet in tweets]\n",
    "        \n",
    "        #join to a form digestible for CountVectorizer\n",
    "        tweets = [' '.join(tweet) for tweet in tweets]\n",
    "        \n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define functions to automate hyperparameter tuning and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function for tuning hyperparams of a classifiers\n",
    "\n",
    "def tune_and_fit(clf, p_grid, cv = 5, X_train = X_train, y_train = y_train):\n",
    "    '''Performs grid search over p_grid with cv-fold cross validation.\n",
    "    \n",
    "    Args:\n",
    "        clf: sklearn classifier\n",
    "        p_grid (dict): parameters grid\n",
    "        cv (int): number of folds in cross validation\n",
    "        X_train (array): training set\n",
    "        y_train (array): training labels\n",
    "        \n",
    "    Returns:\n",
    "        best model found, fitted on the entire data set\n",
    "    '''\n",
    "    \n",
    "    grid = GridSearchCV(clf, param_grid = p_grid, cv = cv, n_jobs = -1, verbose = True, \n",
    "                        scoring = make_scorer(f1_score, average='weighted'))\n",
    "    grid.fit(X_train,y_train)\n",
    "    print('Best parameters found: ')\n",
    "    print(grid.best_params_)\n",
    "    print('\\n Best score:')\n",
    "    print(grid.best_score_)\n",
    "    \n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to evalute tuned models on the test set\n",
    "\n",
    "def evaluate_model(model, X_test = X_test, y_test = y_test):\n",
    "    '''\n",
    "    Args:\n",
    "        model: fitted sklearn classifier\n",
    "        X_test (array): testing set\n",
    "        y_test (array): testing labels\n",
    "    '''\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below pipeline will constitute the feature selection part of every model we create. In short, it uses all four of our custom classes to extract various features, then performs count vectorization on each set of features and finally unions them up using FeatureUnion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A note on vectorization:* by running some simple pipelines with MultinomialBayes classifier we established CountVectorizer to be much more effective than TfidfVectorizer, hence our choice of the vectorizer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this pipeline will be be a feature engineering part of every pipeline we produce\n",
    "# CountVectorizer uses lowercase = False to save time on unecessary lowercasing step: \n",
    "# it is either irrelevant or already done\n",
    "\n",
    "feature_selection = Pipeline([\n",
    "('union',FeatureUnion(transformer_list=[\n",
    "\n",
    "    #pipeline for extracing exclamation marks\n",
    "    ('!', Pipeline([\n",
    "        ('ext', ExclamationMarksExtractor()), #ext = extractor\n",
    "        ('vct', CountVectorizer(token_pattern = r'\\S+', lowercase = False)) #vct = vectorizer\n",
    "    ])),\n",
    "    \n",
    "    #pipeline for extracting keybord emojis\n",
    "    ('key_emoji', Pipeline([\n",
    "        ('ext', KeyboardEmojisExtractor()),\n",
    "        ('vct', CountVectorizer(token_pattern = r'\\S+', lowercase = False))\n",
    "    ])),\n",
    "    \n",
    "    #pipeline for exracting graphical emojis\n",
    "    ('graph_emoji', Pipeline([\n",
    "        ('ext', GraphicsEmojisExtractor()),\n",
    "        ('vct', CountVectorizer(token_pattern = get_emoji_regexp(), lowercase = False))\n",
    "    ])),\n",
    "    \n",
    "    #pipeline for standard bag-of-words model for body of the tweets\n",
    "    ('body', Pipeline([\n",
    "        ('ext', TextCleaner()),\n",
    "        ('vct', CountVectorizer(lowercase = False))\n",
    "    ]))\n",
    "]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipelines will consist of the feature selection pipeline as above, followed by a classifier. The function below automates the process of adjoining a classifier onto a feature selection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_clf(clf, name = 'clf'):\n",
    "    '''Appends a classifier to the feature selection pipeline.\n",
    "    Args:\n",
    "        clf: sklearn classifier\n",
    "        name (str): name of the classifier in the pipeline\n",
    "    '''\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('fts', feature_selection), #fts - features\n",
    "        (name, clf)\n",
    "        ])\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start looking for the best model, let's look at two baselines. Firstly, we will do a simple count vectorization of raw tweets (no preprocessing) and then run a multinomial bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.96      0.86      1836\n",
      "          1       0.81      0.50      0.62       472\n",
      "          2       0.70      0.41      0.52       620\n",
      "\n",
      "avg / total       0.77      0.77      0.75      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline1 = Pipeline([\n",
    "    ('vct', CountVectorizer()),\n",
    "    ('clf',MultinomialNB())\n",
    "    ])\n",
    "\n",
    "baseline1.fit(X_train,y_train)\n",
    "evaluate_model(baseline1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad result for virtually no work! Let's see if extracting emojis and exclamation marks, coupled with more careful text cleaning improves on that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.94      0.86      1836\n",
      "          1       0.78      0.63      0.69       472\n",
      "          2       0.67      0.41      0.51       620\n",
      "\n",
      "avg / total       0.77      0.78      0.76      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline2 = add_clf(MultinomialNB())\n",
    "baseline2.fit(X_train,y_train)\n",
    "evaluate_model(baseline2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the effort with the classes, a more complex pipeline and a longer training time, f1 score improvement is so small it might as well be a fluke. We will nonetheless stick to this more complicated pipeline in a hope that more sophisticated classifiers will be able to use the extra features extracted by our custom classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now optimize a number of popular classifiers and test their performance on the test set. We first run grid searches with 'sparse' sets of parameters, and later narrow down our search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "maxent = add_clf(LogisticRegression(random_state = 0),'maxent')\n",
    "\n",
    "# create a paramter grid\n",
    "maxent_grid = {'maxent__C' : 10**np.arange(-3,4, dtype = float), \n",
    "                   'maxent__penalty' : ['l1','l2'], 'maxent__class_weight' : ['balanced',None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.91      0.88      1836\n",
      "          1       0.79      0.72      0.75       472\n",
      "          2       0.65      0.56      0.60       620\n",
      "\n",
      "avg / total       0.80      0.80      0.80      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the performance of out of the box logistic regression\n",
    "maxent_baseline = add_clf(LogisticRegression(random_state = 0),'maxent')\n",
    "maxent_baseline.fit(X_train,y_train)\n",
    "evaluate_model(maxent_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimize hyperparameters\n",
    "maxent = tune_and_fit(maxent,maxent_grid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best parameters found: \n",
    "# {'maxent__C': 1.0, 'maxent__class_weight': None, 'maxent__penalty': 'l2'}\n",
    "\n",
    "# Best score:\n",
    "# 0.778322037165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.91      0.88      1836\n",
      "          1       0.79      0.72      0.75       472\n",
      "          2       0.65      0.56      0.60       620\n",
      "\n",
      "avg / total       0.80      0.80      0.80      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate optimized model on test set\n",
    "evaluate_model(maxent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that class weight and penalty should remain as default. Let's try and narrow down our search for optimal value of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: \n",
      "{'maxent__C': 1.0}\n",
      "\n",
      " Best score:\n",
      "0.778322037165\n"
     ]
    }
   ],
   "source": [
    "# define a new grid and search it\n",
    "maxent = add_clf(LogisticRegression(random_state = 0),'maxent')\n",
    "maxent_grid = {'maxent__C' : np.array([0.5,1,2,3,4,5])}\n",
    "maxent = tune_and_fit(maxent,maxent_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.91      0.88      1836\n",
      "          1       0.79      0.72      0.75       472\n",
      "          2       0.65      0.56      0.60       620\n",
      "\n",
      "avg / total       0.80      0.80      0.80      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate optimized model on test set\n",
    "evaluate_model(maxent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like default parameters for Logistic Regression work best for our task. Let's pickle the best logistic regression model obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joblib.dump(maxent, 'maxent.pkl');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "svm = add_clf(SVC(random_state = 0, kernel='linear'),'svm')\n",
    "\n",
    "# create a parameter grid\n",
    "svm_grid = {'svm__C' : 10**np.arange(-3,4, dtype = float), \n",
    "                'svm__kernel' : ['linear','rbf','poly'],\n",
    "                'svm__class_weight' : ['balanced',None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.86      0.85      1836\n",
      "          1       0.74      0.70      0.72       472\n",
      "          2       0.59      0.56      0.57       620\n",
      "\n",
      "avg / total       0.77      0.78      0.77      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the performance of out of the box SVM\n",
    "svm_baseline = add_clf(SVC(random_state = 0, kernel='linear'),'svm')\n",
    "svm_baseline.fit(X_train,y_train)\n",
    "evaluate_model(svm_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# optimize hyperparameters\n",
    "svm = tune_and_fit(svm, svm_grid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best parameters found: \n",
    "# {'svm__C': 1000.0, 'svm__class_weight': None, 'svm__kernel': 'rbf'}\n",
    "\n",
    "#  Best score:\n",
    "# 0.772769928666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.88      0.87      1836\n",
      "          1       0.76      0.69      0.72       472\n",
      "          2       0.61      0.58      0.60       620\n",
      "\n",
      "avg / total       0.78      0.79      0.79      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "evaluate_model(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the optimal value of C is at the boundary of our search space, let's extend it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "svm = add_clf(SVC(random_state = 0),'svm') #kernel = rbf\n",
    "\n",
    "# create a parameter grid\n",
    "svm_grid = {'svm__C' : 10**np.arange(3,6,1, dtype = float)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  4.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: \n",
      "{'svm__C': 1000.0}\n",
      "\n",
      " Best score:\n",
      "0.772769928666\n"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters\n",
    "svm = tune_and_fit(svm, svm_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.88      0.87      1836\n",
      "          1       0.76      0.69      0.72       472\n",
      "          2       0.61      0.58      0.60       620\n",
      "\n",
      "avg / total       0.78      0.79      0.79      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "evaluate_model(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best value of C remained 1000. Let's search if there is a better value in the vicinity of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "svm = add_clf(SVC(random_state = 0),'svm') #kernel = rbf\n",
    "\n",
    "# create a parameter grid\n",
    "svm_grid = {'svm__C' : np.arange(500,1600,100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 11 candidates, totalling 55 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed: 10.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: \n",
      "{'svm__C': 900}\n",
      "\n",
      " Best score:\n",
      "0.773973569851\n"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters\n",
    "svm = tune_and_fit(svm, svm_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.89      0.87      1836\n",
      "          1       0.76      0.68      0.72       472\n",
      "          2       0.61      0.57      0.59       620\n",
      "\n",
      "avg / total       0.78      0.79      0.78      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "evaluate_model(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our best value of C is between 900 and 1000. Let's find it to the neareast 10!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "svm = add_clf(SVC(random_state = 0),'svm') #kernel = rbf\n",
    "\n",
    "# create a parameter grid\n",
    "svm_grid = {'svm__C' : np.arange(900,1000,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: \n",
      "{'svm__C': 900}\n",
      "\n",
      " Best score:\n",
      "0.773973569851\n"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters\n",
    "svm = tune_and_fit(svm, svm_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.89      0.87      1836\n",
      "          1       0.76      0.68      0.72       472\n",
      "          2       0.61      0.57      0.59       620\n",
      "\n",
      "avg / total       0.78      0.79      0.78      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "evaluate_model(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 900 it is! Let's pickle our best SVM model for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(svm, 'svm.pkl');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "forest = add_clf(RandomForestClassifier(random_state = 0), 'forest')\n",
    "\n",
    "# create parameter grids\n",
    "forest_grid = {'forest__max_depth' : [10, 50, 100],\n",
    "                'forest__n_estimators': [100, 500],\n",
    "                'forest__min_samples_leaf': [10, 50, 100],\n",
    "                'forest__class_weight' : ['balanced',None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.89      0.84      1836\n",
      "          1       0.70      0.62      0.66       472\n",
      "          2       0.59      0.41      0.48       620\n",
      "\n",
      "avg / total       0.73      0.75      0.73      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the performance of out of the box RandomForest\n",
    "forest_baseline = add_clf(RandomForestClassifier(random_state = 0), 'forest')\n",
    "forest_baseline.fit(X_train,y_train)\n",
    "evaluate_model(forest_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimize hyperparameters\n",
    "forest = tune_and_fit(forest,forest_grid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best parameters found: \n",
    "# {'forest__class_weight': 'balanced', 'forest__max_depth': 100, 'forest__min_samples_leaf': 10, 'forest__n_estimators': 500}\n",
    "\n",
    "#  Best score:\n",
    "# 0.725788550582"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.72      0.80      1836\n",
      "          1       0.59      0.77      0.67       472\n",
      "          2       0.49      0.65      0.56       620\n",
      "\n",
      "avg / total       0.76      0.72      0.73      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "evaluate_model(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters were at their boundary values, let's extend our search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "forest = add_clf(RandomForestClassifier(random_state = 0, class_weight = 'balanced'), 'forest')\n",
    "\n",
    "# create parameter grids\n",
    "forest_grid = {'forest__max_depth' : [100,200,300],\n",
    "                'forest__n_estimators': [100, 500,1000],\n",
    "                'forest__min_samples_leaf': [2,5,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed: 17.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: \n",
      "{'forest__max_depth': 300, 'forest__min_samples_leaf': 2, 'forest__n_estimators': 1000}\n",
      "\n",
      " Best score:\n",
      "0.756624210259\n"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters\n",
    "forest = tune_and_fit(forest,forest_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.79      0.83      1836\n",
      "          1       0.63      0.77      0.69       472\n",
      "          2       0.55      0.61      0.58       620\n",
      "\n",
      "avg / total       0.77      0.75      0.76      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "evaluate_model(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we're at the boundary, so let's look further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "forest = add_clf(RandomForestClassifier(random_state = 0, class_weight='balanced'), 'forest')\n",
    "\n",
    "# create parameter grids\n",
    "forest_grid = {'forest__max_depth' : [300,400,500],\n",
    "                'forest__n_estimators': [1000,1500,2000],\n",
    "               'forest__min_samples_leaf': [1,2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 31.2min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 61.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: \n",
      "{'forest__max_depth': 300, 'forest__min_samples_leaf': 1, 'forest__n_estimators': 1000}\n",
      "\n",
      " Best score:\n",
      "0.758534316104\n"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters\n",
    "forest = tune_and_fit(forest,forest_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.89      0.85      1836\n",
      "          1       0.74      0.66      0.70       472\n",
      "          2       0.62      0.53      0.57       620\n",
      "\n",
      "avg / total       0.77      0.77      0.77      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "evaluate_model(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks like we found the rough region where the best parameters lie. Let's try and narrow them down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "forest = add_clf(RandomForestClassifier(random_state = 0, \n",
    "                class_weight='balanced'), 'forest') #default min samples leaf is 1\n",
    "\n",
    "# create parameter grids\n",
    "forest_grid = {'forest__max_depth' : [250,300,350],\n",
    "                'forest__n_estimators': [900,1000,1100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed: 34.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: \n",
      "{'forest__max_depth': 250, 'forest__n_estimators': 900}\n",
      "\n",
      " Best score:\n",
      "0.758980866869\n"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters\n",
    "forest = tune_and_fit(forest,forest_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.88      0.85      1836\n",
      "          1       0.74      0.66      0.70       472\n",
      "          2       0.62      0.54      0.58       620\n",
      "\n",
      "avg / total       0.77      0.78      0.77      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "evaluate_model(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's narrow down forest's max depth to the nearest 10 and keep searching for optimal number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "forest = add_clf(RandomForestClassifier(random_state = 0, \n",
    "                class_weight='balanced'), 'forest') #default min samples leaf is 1\n",
    "\n",
    "# create parameter grids\n",
    "forest_grid = {'forest__max_depth' : np.arange(250,280,10),\n",
    "                'forest__n_estimators': [700,800,900]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed: 28.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: \n",
      "{'forest__max_depth': 250, 'forest__n_estimators': 700}\n",
      "\n",
      " Best score:\n",
      "0.759013393401\n"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters\n",
    "forest = tune_and_fit(forest,forest_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.88      0.85      1836\n",
      "          1       0.73      0.66      0.70       472\n",
      "          2       0.61      0.53      0.57       620\n",
      "\n",
      "avg / total       0.77      0.77      0.77      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "evaluate_model(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a pipeline\n",
    "forest = add_clf(RandomForestClassifier(random_state = 0, \n",
    "            class_weight='balanced', max_depth = 250), 'forest') #default min samples leaf is 1\n",
    "\n",
    "# create parameter grids\n",
    "forest_grid = {'forest__n_estimators': [600,650,700,750]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  9.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: \n",
      "{'forest__n_estimators': 700}\n",
      "\n",
      " Best score:\n",
      "0.759013393401\n"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters\n",
    "forest = tune_and_fit(forest,forest_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We settle our search at max depth of 250 and 700 estimators. Let's pickle the best forest found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(forest, 'forest.pkl');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have emojis helped?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that our best model is logistic regression. Due to the complextiy of our pipeline, it is difficult to track down which words or emojis played significant roles in our decision function. Let's try and see if including emojis and exclamation marks yielded an improved predictive power by running a logistic regression of features obtained by a simple Count Vectorizer (no preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.96      0.86      1836\n",
      "          1       0.81      0.50      0.62       472\n",
      "          2       0.70      0.41      0.52       620\n",
      "\n",
      "avg / total       0.77      0.77      0.75      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline3 = Pipeline([\n",
    "    ('vct', CountVectorizer()),\n",
    "    ('clf',LogisticRegression())\n",
    "    ])\n",
    "\n",
    "baseline3.fit(X_train,y_train)\n",
    "evaluate_model(baseline1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we obtained a 0.05 improvement of f1 score on the test set with our custom pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONNECTING TO TWITTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test our best model on real world data by fetching some recent tweets and classyfing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to use fetch_and_classify, fill these with your own key and token \n",
    "ACCESS_TOKEN = 'YOUR ACCESS TOKEN'\n",
    "ACCESS_SECRET = 'YOUR ACCESS SECRET'\n",
    "CONSUMER_KEY = 'YOUR CONSUMER KEY'\n",
    "CONSUMER_SECRET = 'YOUR CONSUMER SECRET'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_and_classify(airline, model, language = 'en', n = 10, sentiment = None):\n",
    "    '''\n",
    "    Fetches n recent tweets in language about airline. \n",
    "    Prints tweets together with their sentiment, as classified by the model\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of tweets to fetch\n",
    "        airline (str): airline's twitter username (without @)\n",
    "        language (str): tweets' language, default is English\n",
    "        model (str): path to a trained and pickled sklearn classifier\n",
    "        sentiment (str): one of 'positive', 'negative' or None, defualt None. \n",
    "                        Allows to fetch tweets of a given sentiment only (as per twitter api)\n",
    "        \n",
    "    Returns: \n",
    "        None\n",
    "    '''\n",
    "    #sentiment dict\n",
    "    s_dict = {'positive': ':)', 'negative' : ':('}\n",
    "    \n",
    "    # connect to twitter\n",
    "    oauth = OAuth(ACCESS_TOKEN, ACCESS_SECRET, CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    twitter = Twitter(auth=oauth)\n",
    "    \n",
    "    # fetch tweets\n",
    "    if sentiment:\n",
    "        tweets = twitter.search.tweets(q = '@{} {} -filter:retweets'.format(airline,\n",
    "                s_dict[sentiment]), result_type = 'recent', lang = 'en', \n",
    "                count = n, tweet_mode = 'extended')\n",
    "    else:\n",
    "        tweets = twitter.search.tweets(q = '@{} -filter:retweets'.format(airline), \n",
    "                result_type = 'recent', lang = 'en', count = n, tweet_mode = 'extended')\n",
    "    \n",
    "    # extract text\n",
    "    tweets = [tweet['full_text'] for tweet in tweets['statuses']]\n",
    "    \n",
    "    # load classifier\n",
    "    clf = joblib.load(model) \n",
    "    \n",
    "    # make predictions\n",
    "    predictions = clf.predict(tweets)\n",
    "    predictions = pd.Series(predictions).map({0 :'negative', 1: 'positive', 2: 'neutral'}).values\n",
    "    \n",
    "    # print the results\n",
    "    for i in range(n):\n",
    "        print(tweets[i])\n",
    "        print('Classifier prediction: {}'.format(predictions[i]))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model handles some recent tweets about United."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@take2review @united That's pretty anticlimactic. I'm disappointed.\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "Yo @united your basic economy rules scares me can I bring a backpack on? What's considered full size carry on?\n",
      "Classifier prediction: neutral\n",
      "\n",
      "\n",
      "@united It's only U.S., not worldwide.\n",
      "Classifier prediction: neutral\n",
      "\n",
      "\n",
      "@fortis_semper @united did @united get back to you? @seatguru shows United's 767-300 row 42 \"limited or no recline,\" passengers claim no recline in that row https://t.co/bMcze6EBlI\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "Second flight of the day. Boarded @united flight UA6313 IAD - YUL. Getting closer to @NEPHP #nephp17 https://t.co/pkpsegxxsF\n",
      "Classifier prediction: neutral\n",
      "\n",
      "\n",
      "@mandakayrocks @united Reporting in.  Nothing happened.  Lol. I never saw him again.\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "@united Do you think Dao would love that paradise? https://t.co/PT17S2mwSp\n",
      "Classifier prediction: neutral\n",
      "\n",
      "\n",
      "@united  today you rock. Capt Dave Harvey got flt 2063 down safe and CLE crew is amazing. https://t.co/HNpKhM25Qr\n",
      "Classifier prediction: positive\n",
      "\n",
      "\n",
      "@united So doctors and pets are not safe flying on your planes. United, you are a complete shitshow.\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "@arthurmessal @united the trick, Mr. Potter, is to perform the cabin service without MINDING that it's a chore (something they have yet to learn)\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fetch_and_classify('United','maxent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave it up to you to decide if the classification is good enough, but we're satisfied :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most tweets aimed at airlines are negative and we had the highest proportion of such in our training set, these are the easiest to classify. Let's see how our classifier deals with tweets which Twitter API considers positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks to @VirginAmerica's inaugural SFO/SEA fares. @betterinrealife and I were able to survive a LDR phase. And now, look at us. :) HBD VA! https://t.co/dERAenBjNv\n",
      "Classifier prediction: positive\n",
      "\n",
      "\n",
      "On a @VirginAmerica flight, of all the movies my 8yo could have selected to watch, she decided to re-watch @HiddenFigures :)\n",
      "Classifier prediction: neutral\n",
      "\n",
      "\n",
      "@VirginAmerica Ok thanks. No worries. Just making sure.  Heading to airport soon. :)\n",
      "Classifier prediction: positive\n",
      "\n",
      "\n",
      "Maybe the one time I've responded to a corporate newsletter. Could not resist @VirginAmerica. Or could I? :) https://t.co/0xLXwTwj4f\n",
      "Classifier prediction: neutral\n",
      "\n",
      "\n",
      "@VirginAmerica flight 221 looks like there are 2 open 1st class and my wife and I need a free upgrade.:) 11a11b #donthurttoask\n",
      "Classifier prediction: neutral\n",
      "\n",
      "\n",
      "@VetCopGOP @AnnCoulter @Delta @VirginAmerica @JetBlue glad to know we've got good people like you out there protecting us, and protecting us from ourselves :)\n",
      "Classifier prediction: positive\n",
      "\n",
      "\n",
      "@VetCopGOP @AnnCoulter @Delta @VirginAmerica @JetBlue and all the best to you and your family as well! always glad to speak with one of the good ones :)\n",
      "Classifier prediction: positive\n",
      "\n",
      "\n",
      "@VetCopGOP @AnnCoulter @Delta @VirginAmerica @JetBlue you too, Brian! Again, to make clear, never assumed you were a bad one, or that any are until proven otherwise. Hope you follow the same :)\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "@VirginAmerica already tried that, he couldn't help. he told us totally different stuff that the guy at the counter told us. now we're stuck in LA :)\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "@VirginAmerica prefer booking online. will it actually be fixed or deterrent so we would start using alaska asap? :)\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fetch_and_classify('VirginAmerica','maxent.pkl', sentiment = 'positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, some of the tweets Twitter considered positive were actually the opposite, and our classifier has caught that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, let's see how our classifier deals with an airline which was not present in our training set.** We will use Ryanair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Ryanair My flight from STN to SXF is listed as on time, but the plane isn't even here yet. Could you give an update? (FR8544)\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "Fly for Stanstead direct to Genoa from as little as Â£15 each way! A good reason to #learntosail in Italy. @Ryanair https://t.co/VUb6xbkGxt\n",
      "Classifier prediction: positive\n",
      "\n",
      "\n",
      "@Ryanair Done thank you\n",
      "Classifier prediction: positive\n",
      "\n",
      "\n",
      "@Ryanair flight to corfu 1 hr late. No apology. People sitting on flight. Awful.\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "@saratctravel @Flight_Refunds @Ryanair Where in the hell were you flying to? Mars? Jings oh!\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "Congratulations @Ryanair for great idea of uniformly distributing families along plane. Outcome: delay until people manage to switch places.\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "@Ryanair Flight 4195 is now expected to be at 10pm instead of 14.30. You planning on having anyone in the airport to explain what's going on?\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "Reasons NOT to fly @Ryanair: \n",
      "\n",
      "1. Complete and utter shite.\n",
      "\n",
      "The end.\n",
      "\n",
      "Never again.\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "@Ryanair @RyanairFlights my cabin luggage is 55x36x24, is this too large?\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n",
      "@Ryanair can you help tell us why our flight is delayed &amp; what time we should expect to fly? FR1875 - cannot get an answer\n",
      "Classifier prediction: negative\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fetch_and_classify('Ryanair', 'maxent.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we leave it up to you to gauge the quality of the predictions.\n",
    "** Finally, fell free to play around with pickled classifiers and our tweets-fetching function!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sidenote*: Interestingly, none of the fetched tweets contained any emojis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task was to create a robust sentiment classifier and test it in the field by fetching some recent, real world tweets. We accomplished our task by creating custom feature extraction classes: extracting exclamation marks, two types of emojis and finally cleaning, tokenizing and stemming text. Subsequently, multi stage pipelines were created and a number of classifiers were tuned and fitted. The best (according to f1 score) classifier was logistic regression, closely followed by SVM with gaussian kernel. Surprisingly, Random Forests underperformed. \n",
    "\n",
    "Even though a task of testing our classifier with real world tweets was completed, our logistic regression classifier still leaves room for improvement. Let's list some of potential strategies we could have used aimed at improving our f1 score:\n",
    "\n",
    "* using a different type of model, eq: xgboost, neural networks\n",
    "* incorporating ngrams: quick and dirty testing with multinomial NB showed that using ngrams actually decreased predictive power, but perhaps using 2-grams with a specific combination of parameters would have yielded better scores. Testing this would have produced much larger parameter grids, though\n",
    "* incorporating different weights to different parts of our feature extraction, say, weighing emojis twice as important as words: as above, quick and dirty check revealed this to either not change anything or actually decrease performance. Again, a more systematic way to check this would be to use grid search, but computational costs were prohibitive for a small laptop\n",
    "* using ensemble methods: boosting, bagging, stacking?\n",
    "\n",
    "Finally, let's note that our aim was not to max out our f1 score at all cost. The main purpose of this exercises was to learn and practice writing custom sklearn compatible classes, use them together with sklearn pipelines and grid searches and have a perhaps imperfect, but decently working classifier with which we can connect to twitter and classify some actual tweets. We consider this mission accomplished and we learned loads during the process :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
